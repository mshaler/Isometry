---
phase: 69-file-importers
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/etl/importers/MarkdownImporter.ts
  - src/etl/__tests__/MarkdownImporter.test.ts
autonomous: true

must_haves:
  truths:
    - "MarkdownImporter parses frontmatter and content from .md files"
    - "Frontmatter maps to LATCH fields with flexible key detection"
    - "Markdown body converts to HTML and stores in content field"
    - "Deterministic sourceId enables reimport without duplicates"
  artifacts:
    - path: "src/etl/importers/MarkdownImporter.ts"
      provides: "Markdown importer extending BaseImporter"
      exports: ["MarkdownImporter"]
    - path: "src/etl/__tests__/MarkdownImporter.test.ts"
      provides: "TDD tests for markdown import"
      min_lines: 80
  key_links:
    - from: "src/etl/importers/MarkdownImporter.ts"
      to: "src/etl/importers/BaseImporter.ts"
      via: "extends BaseImporter"
      pattern: "extends BaseImporter"
    - from: "src/etl/importers/MarkdownImporter.ts"
      to: "src/etl/id-generation/deterministic.ts"
      via: "generateDeterministicSourceId"
      pattern: "generateDeterministicSourceId"
---

<objective>
Implement MarkdownImporter for .md file import using TDD.

Purpose: Markdown is the most common format, gray-matter is already installed, and alto-importer.ts provides a proven reference pattern. This establishes the pattern for all other importers.

Output: Working MarkdownImporter that parses frontmatter + body, maps to CanonicalNode, and generates deterministic IDs.
</objective>

<execution_context>
@/Users/mshaler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mshaler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/68-import-coordinator/68-01-SUMMARY.md

# Reference pattern for frontmatter parsing and ID generation
@src/etl/alto-importer.ts
@src/etl/alto-parser.ts

# Foundation from Phase 68
@src/etl/importers/BaseImporter.ts
@src/etl/types/canonical.ts
@src/etl/id-generation/deterministic.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install marked and types, create failing tests</name>
  <files>
    package.json
    src/etl/__tests__/MarkdownImporter.test.ts
  </files>
  <action>
1. Install marked for markdown-to-HTML conversion:
   ```bash
   npm install marked
   npm install -D @types/marked
   ```

2. Create MarkdownImporter.test.ts with failing tests covering:
   - Basic markdown with frontmatter (title, tags, folder, created)
   - Markdown without frontmatter (extracts title from first H1)
   - LATCH field mapping from various frontmatter keys:
     - Time: created, modified, due, created_at, createdAt (flexible)
     - Category: tags (array or comma-separated string), folder, status
     - Hierarchy: priority (string or number), importance
   - Deterministic sourceId generation (same input = same sourceId)
   - Extended properties stored for unknown frontmatter keys
   - Empty markdown file returns valid node with defaults
   - HTML conversion preserves formatting

3. Test pattern (TDD RED phase):
   ```typescript
   import { MarkdownImporter } from '../importers/MarkdownImporter';
   import { CanonicalNodeSchema } from '../types/canonical';
   import { FileSource } from '../importers/BaseImporter';

   describe('MarkdownImporter', () => {
     let importer: MarkdownImporter;

     beforeEach(() => {
       importer = new MarkdownImporter();
     });

     it('should parse frontmatter and content', async () => {
       const source: FileSource = {
         filename: 'test.md',
         content: `---
title: Test Note
tags: [work, important]
created: 2024-01-15T10:00:00Z
---

# Test Note

This is the body content.`,
       };

       const nodes = await importer.import(source);

       expect(nodes).toHaveLength(1);
       expect(nodes[0].name).toBe('Test Note');
       expect(nodes[0].tags).toEqual(['work', 'important']);
       expect(nodes[0].createdAt).toBe('2024-01-15T10:00:00Z');
       expect(nodes[0].content).toContain('<h1');
     });

     it('should extract title from H1 when no frontmatter title', async () => {
       const source: FileSource = {
         filename: 'no-title.md',
         content: `# Extracted Title

Body text.`,
       };

       const nodes = await importer.import(source);

       expect(nodes[0].name).toBe('Extracted Title');
     });

     it('should generate deterministic sourceId', async () => {
       const source: FileSource = {
         filename: 'deterministic.md',
         content: `---
title: Deterministic Test
---

Content.`,
       };

       const nodes1 = await importer.import(source);
       const nodes2 = await importer.import(source);

       expect(nodes1[0].sourceId).toBe(nodes2[0].sourceId);
       expect(nodes1[0].sourceId).toMatch(/^markdown-importer-/);
     });

     // Additional tests for flexible field detection, validation, etc.
   });
   ```

4. Run tests to verify they fail (RED phase):
   ```bash
   npm run test -- --run src/etl/__tests__/MarkdownImporter.test.ts
   ```
  </action>
  <verify>
    - `npm run test -- --run src/etl/__tests__/MarkdownImporter.test.ts` runs and all tests FAIL (no MarkdownImporter yet)
    - marked package installed (check package.json)
  </verify>
  <done>Tests written for MarkdownImporter covering frontmatter parsing, LATCH mapping, deterministic IDs, and edge cases. All tests fail as expected (RED phase).</done>
</task>

<task type="auto">
  <name>Task 2: Implement MarkdownImporter to pass tests</name>
  <files>src/etl/importers/MarkdownImporter.ts</files>
  <action>
1. Create MarkdownImporter extending BaseImporter:

```typescript
/**
 * Markdown Importer for Isometry ETL
 *
 * Parses .md files with optional YAML frontmatter.
 * Uses gray-matter for frontmatter and marked for HTML conversion.
 *
 * LATCH mapping:
 * - L: locationName, locationAddress from frontmatter
 * - A: name from title/first H1
 * - T: createdAt, modifiedAt, dueAt from flexible key detection
 * - C: folder, tags, status from frontmatter
 * - H: priority, importance from frontmatter
 */

import matter from 'gray-matter';
import { marked } from 'marked';
import { v4 as uuidv4 } from 'uuid';
import { BaseImporter, FileSource } from './BaseImporter';
import { CanonicalNode } from '../types/canonical';
import { generateDeterministicSourceId } from '../id-generation/deterministic';

interface ParsedMarkdown {
  frontmatter: Record<string, unknown>;
  html: string;
  raw: string;
  filename: string;
}

export class MarkdownImporter extends BaseImporter {
  protected async parse(source: FileSource): Promise<unknown> {
    const parsed = matter(source.content);
    const html = await marked.parse(parsed.content);

    return {
      frontmatter: parsed.data,
      html,
      raw: parsed.content,
      filename: source.filename,
    };
  }

  protected async transform(data: unknown): Promise<CanonicalNode[]> {
    const { frontmatter, html, raw, filename } = data as ParsedMarkdown;
    const now = new Date().toISOString();

    const sourceId = generateDeterministicSourceId(
      filename,
      frontmatter,
      'markdown-importer'
    );

    const node: CanonicalNode = {
      id: uuidv4(),
      sourceId,
      source: 'markdown-importer',
      nodeType: detectNodeType(frontmatter),
      name: detectName(frontmatter, raw),
      content: html,
      summary: detectSummary(frontmatter, raw),

      // LATCH: Location
      latitude: null,
      longitude: null,
      locationName: (frontmatter.location_name || frontmatter.locationName) as string | null ?? null,
      locationAddress: (frontmatter.location || frontmatter.address || frontmatter.locationAddress) as string | null ?? null,

      // LATCH: Time (flexible key detection)
      createdAt: detectDate(frontmatter, ['created', 'createdAt', 'created_at', 'date']) || now,
      modifiedAt: detectDate(frontmatter, ['modified', 'modifiedAt', 'modified_at', 'updated']) || now,
      dueAt: detectDate(frontmatter, ['due', 'dueAt', 'due_at', 'deadline']) || null,
      completedAt: detectDate(frontmatter, ['completed', 'completedAt', 'completed_at']) || null,
      eventStart: detectDate(frontmatter, ['start', 'eventStart', 'event_start', 'start_date']) || null,
      eventEnd: detectDate(frontmatter, ['end', 'eventEnd', 'event_end', 'end_date']) || null,

      // LATCH: Category
      folder: (frontmatter.folder || frontmatter.category) as string | null ?? null,
      tags: detectTags(frontmatter),
      status: (frontmatter.status || frontmatter.state) as string | null ?? null,

      // LATCH: Hierarchy
      priority: detectPriority(frontmatter),
      importance: (frontmatter.importance as number) || 0,
      sortOrder: (frontmatter.sort_order || frontmatter.sortOrder || 0) as number,

      // Grid
      gridX: 0,
      gridY: 0,

      // Provenance
      sourceUrl: (frontmatter.url || frontmatter.source_url || frontmatter.link) as string | null ?? null,
      deletedAt: null,
      version: 1,

      // Extended properties for unknown keys
      properties: extractUnknownProperties(frontmatter),
    };

    return [node];
  }
}

// Helper functions for flexible field detection
function detectNodeType(fm: Record<string, unknown>): string {
  return (fm.type || fm.nodeType || fm.node_type || 'note') as string;
}

function detectName(fm: Record<string, unknown>, raw: string): string {
  if (fm.title) return fm.title as string;
  if (fm.name) return fm.name as string;

  // Extract from first H1
  const h1Match = raw.match(/^#\s+(.+)$/m);
  if (h1Match) return h1Match[1];

  return 'Untitled';
}

function detectSummary(fm: Record<string, unknown>, raw: string): string | null {
  if (fm.summary) return fm.summary as string;
  if (fm.description) return fm.description as string;

  // First non-heading, non-empty line (max 200 chars)
  const lines = raw.split('\n').filter(line => {
    const t = line.trim();
    return t && !t.startsWith('#') && !t.startsWith('---');
  });
  return lines[0]?.slice(0, 200) || null;
}

function detectDate(fm: Record<string, unknown>, keys: string[]): string | null {
  for (const key of keys) {
    const val = fm[key];
    if (val) {
      // Handle Date objects
      if (val instanceof Date) return val.toISOString();
      // Handle strings - ensure ISO format
      if (typeof val === 'string') {
        // If already ISO, return as-is
        if (/^\d{4}-\d{2}-\d{2}T/.test(val)) return val;
        // Try to parse and convert
        const parsed = new Date(val);
        if (!isNaN(parsed.getTime())) return parsed.toISOString();
      }
    }
  }
  return null;
}

function detectTags(fm: Record<string, unknown>): string[] {
  const tags = fm.tags || fm.labels || fm.categories;
  if (!tags) return [];
  if (Array.isArray(tags)) return tags.map(String);
  if (typeof tags === 'string') {
    return tags.split(/[,;]/).map(t => t.trim()).filter(Boolean);
  }
  return [];
}

function detectPriority(fm: Record<string, unknown>): number {
  const p = fm.priority;
  if (typeof p === 'number') return Math.min(5, Math.max(0, p));
  if (typeof p === 'string') {
    const lower = p.toLowerCase();
    if (lower === 'high' || lower === 'urgent') return 5;
    if (lower === 'medium' || lower === 'normal') return 3;
    if (lower === 'low') return 1;
    const num = parseInt(p, 10);
    if (!isNaN(num)) return Math.min(5, Math.max(0, num));
  }
  return 0;
}

// Known LATCH keys that map to CanonicalNode fields
const KNOWN_KEYS = new Set([
  'title', 'name', 'type', 'nodeType', 'node_type',
  'content', 'summary', 'description',
  'latitude', 'longitude', 'location', 'location_name', 'locationName',
  'address', 'locationAddress', 'location_address',
  'created', 'createdAt', 'created_at', 'date',
  'modified', 'modifiedAt', 'modified_at', 'updated',
  'due', 'dueAt', 'due_at', 'deadline',
  'completed', 'completedAt', 'completed_at',
  'start', 'eventStart', 'event_start', 'start_date',
  'end', 'eventEnd', 'event_end', 'end_date',
  'folder', 'category',
  'tags', 'labels', 'categories',
  'status', 'state',
  'priority', 'importance',
  'sort_order', 'sortOrder',
  'url', 'source_url', 'link',
]);

function extractUnknownProperties(fm: Record<string, unknown>): Record<string, unknown> {
  const props: Record<string, unknown> = {
    originalFormat: 'markdown',
  };

  for (const [key, value] of Object.entries(fm)) {
    if (!KNOWN_KEYS.has(key)) {
      props[key] = value;
    }
  }

  return props;
}
```

2. Run tests to verify GREEN:
   ```bash
   npm run test -- --run src/etl/__tests__/MarkdownImporter.test.ts
   ```

3. Run typecheck:
   ```bash
   npm run check:types
   ```
  </action>
  <verify>
    - `npm run test -- --run src/etl/__tests__/MarkdownImporter.test.ts` passes all tests (GREEN)
    - `npm run check:types` has zero errors
    - MarkdownImporter extends BaseImporter correctly
  </verify>
  <done>MarkdownImporter implemented and all tests pass. Parses frontmatter, converts to HTML, maps LATCH fields, generates deterministic IDs.</done>
</task>

<task type="auto">
  <name>Task 3: Register importer and verify integration</name>
  <files>src/etl/__tests__/MarkdownImporter.test.ts</files>
  <action>
1. Add integration test verifying MarkdownImporter works with ImportCoordinator:

```typescript
describe('MarkdownImporter integration', () => {
  it('should integrate with ImportCoordinator', async () => {
    const { ImportCoordinator } = await import('../coordinator/ImportCoordinator');
    const coordinator = new ImportCoordinator();
    coordinator.registerImporter(['.md', '.markdown', '.mdx'], new MarkdownImporter());

    const source: FileSource = {
      filename: 'integration-test.md',
      content: `---
title: Integration Test
---

Content here.`,
    };

    const nodes = await coordinator.importFile(source);

    expect(nodes).toHaveLength(1);
    expect(nodes[0].name).toBe('Integration Test');
    // Validates against CanonicalNodeSchema
    expect(() => CanonicalNodeSchema.parse(nodes[0])).not.toThrow();
  });
});
```

2. Run full test suite to ensure no regressions:
   ```bash
   npm run test -- --run
   ```

3. Run quality checks:
   ```bash
   npm run check:types
   npm run check:lint
   ```
  </action>
  <verify>
    - All MarkdownImporter tests pass
    - Integration with ImportCoordinator works
    - All nodes pass CanonicalNodeSchema validation
    - `npm run check:types` and `npm run check:lint` pass
  </verify>
  <done>MarkdownImporter complete with full test coverage and ImportCoordinator integration verified.</done>
</task>

</tasks>

<verification>
1. `npm run test -- --run src/etl/__tests__/MarkdownImporter.test.ts` - all tests pass
2. `npm run check:types` - zero TypeScript errors
3. MarkdownImporter produces valid CanonicalNode[] verified by Zod schema
4. marked package installed and working
</verification>

<success_criteria>
1. MarkdownImporter parses .md files with frontmatter using gray-matter
2. Markdown body converted to HTML using marked
3. LATCH fields mapped with flexible key detection
4. Deterministic sourceId generated for deduplication
5. Unknown frontmatter keys stored in properties
6. All tests pass with TDD RED-GREEN-REFACTOR cycle
7. Integration with ImportCoordinator verified
</success_criteria>

<output>
After completion, create `.planning/phases/69-file-importers/69-01-SUMMARY.md`
</output>
