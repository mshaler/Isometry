---
phase: 69-file-importers
plan: 05
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/etl/importers/WordImporter.ts
  - src/etl/__tests__/WordImporter.test.ts
autonomous: true

must_haves:
  truths:
    - "WordImporter extracts text content from .docx files"
    - "Document converted to semantic HTML using mammoth"
    - "Title extracted from first heading"
    - "Binary content handled via base64 encoding"
  artifacts:
    - path: "src/etl/importers/WordImporter.ts"
      provides: "Word importer extending BaseImporter"
      exports: ["WordImporter"]
    - path: "src/etl/__tests__/WordImporter.test.ts"
      provides: "TDD tests for Word import"
      min_lines: 60
  key_links:
    - from: "src/etl/importers/WordImporter.ts"
      to: "src/etl/importers/BaseImporter.ts"
      via: "extends BaseImporter"
      pattern: "extends BaseImporter"
    - from: "src/etl/importers/WordImporter.ts"
      to: "mammoth"
      via: "mammoth.convertToHtml"
      pattern: "mammoth\\.convertToHtml"
---

<objective>
Implement WordImporter for .docx file import using TDD.

Purpose: Word documents are common in enterprise contexts. Mammoth is already installed and converts DOCX to semantic HTML, preserving formatting while extracting readable content.

Output: Working WordImporter that converts DOCX to HTML and maps to CanonicalNode.
</objective>

<execution_context>
@/Users/mshaler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mshaler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/68-import-coordinator/68-01-SUMMARY.md

# Foundation from Phase 68
@src/etl/importers/BaseImporter.ts
@src/etl/types/canonical.ts
@src/etl/id-generation/deterministic.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create failing tests for WordImporter</name>
  <files>src/etl/__tests__/WordImporter.test.ts</files>
  <action>
1. Create WordImporter.test.ts with failing tests:

Note: Testing DOCX requires either:
- Creating minimal DOCX files programmatically (complex)
- Mocking mammoth (simpler for unit tests)
- Integration tests with real DOCX files

For TDD, we'll mock mammoth and test the transformation logic:

```typescript
import { WordImporter } from '../importers/WordImporter';
import { CanonicalNodeSchema } from '../types/canonical';
import { FileSource } from '../importers/BaseImporter';
import { vi, describe, it, expect, beforeEach, afterEach } from 'vitest';

// Mock mammoth
vi.mock('mammoth', () => ({
  default: {
    convertToHtml: vi.fn(),
  },
}));

import mammoth from 'mammoth';

describe('WordImporter', () => {
  let importer: WordImporter;

  beforeEach(() => {
    importer = new WordImporter();
  });

  afterEach(() => {
    vi.clearAllMocks();
  });

  describe('basic parsing', () => {
    it('should convert DOCX to HTML', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Document Title</h1><p>Document content here.</p>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'test.docx',
        content: 'base64-encoded-content',
        encoding: 'base64',
      });

      expect(nodes).toHaveLength(1);
      expect(nodes[0].content).toContain('<h1>Document Title</h1>');
      expect(nodes[0].content).toContain('<p>Document content');
    });

    it('should extract title from first <h1>', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Extracted Title</h1><p>Content</p>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'titled.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(nodes[0].name).toBe('Extracted Title');
    });

    it('should fallback to filename when no heading', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<p>Just paragraphs, no heading</p>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'no-heading.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(nodes[0].name).toBe('no-heading');
    });
  });

  describe('content handling', () => {
    it('should preserve HTML formatting', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Title</h1><p><strong>Bold</strong> and <em>italic</em></p><ul><li>Item 1</li></ul>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'formatted.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(nodes[0].content).toContain('<strong>Bold</strong>');
      expect(nodes[0].content).toContain('<em>italic</em>');
      expect(nodes[0].content).toContain('<ul>');
    });

    it('should generate summary from first paragraph', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Title</h1><p>This is the first paragraph that should become the summary.</p><p>Second paragraph.</p>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'summary.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(nodes[0].summary).toContain('This is the first paragraph');
    });
  });

  describe('encoding handling', () => {
    it('should handle base64 encoded content', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<p>Content</p>',
        messages: [],
      });

      await importer.import({
        filename: 'base64.docx',
        content: 'SGVsbG8gV29ybGQ=', // "Hello World" in base64
        encoding: 'base64',
      });

      expect(mammoth.convertToHtml).toHaveBeenCalled();
      // Verify buffer was created correctly
      const call = vi.mocked(mammoth.convertToHtml).mock.calls[0];
      expect(call[0]).toHaveProperty('buffer');
    });

    it('should handle UTF-8 string content', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<p>Content</p>',
        messages: [],
      });

      await importer.import({
        filename: 'utf8.docx',
        content: 'string content',
        encoding: 'utf8',
      });

      expect(mammoth.convertToHtml).toHaveBeenCalled();
    });
  });

  describe('conversion warnings', () => {
    it('should store conversion warnings in properties', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<p>Content</p>',
        messages: [
          { type: 'warning', message: 'Unknown element: w:drawing' },
          { type: 'warning', message: 'Unsupported feature' },
        ],
      });

      const nodes = await importer.import({
        filename: 'warnings.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(nodes[0].properties).toHaveProperty('conversionWarnings');
      expect(nodes[0].properties.conversionWarnings).toHaveLength(2);
    });
  });

  describe('deterministic IDs', () => {
    it('should generate deterministic sourceId', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Test</h1><p>Content</p>',
        messages: [],
      });

      const source: FileSource = {
        filename: 'deterministic.docx',
        content: 'base64content',
        encoding: 'base64',
      };

      const nodes1 = await importer.import(source);
      const nodes2 = await importer.import(source);

      expect(nodes1[0].sourceId).toBe(nodes2[0].sourceId);
      expect(nodes1[0].sourceId).toMatch(/^word-importer-/);
    });
  });

  describe('validation', () => {
    it('should produce valid CanonicalNode', async () => {
      vi.mocked(mammoth.convertToHtml).mockResolvedValue({
        value: '<h1>Valid</h1><p>Content</p>',
        messages: [],
      });

      const nodes = await importer.import({
        filename: 'valid.docx',
        content: 'base64content',
        encoding: 'base64',
      });

      expect(() => CanonicalNodeSchema.parse(nodes[0])).not.toThrow();
    });
  });

  describe('error handling', () => {
    it('should throw descriptive error on conversion failure', async () => {
      vi.mocked(mammoth.convertToHtml).mockRejectedValue(
        new Error('Invalid DOCX file')
      );

      await expect(
        importer.import({
          filename: 'invalid.docx',
          content: 'bad-content',
          encoding: 'base64',
        })
      ).rejects.toThrow(/invalid\.docx.*Invalid DOCX/i);
    });
  });
});
```

2. Run tests to verify they fail:
   ```bash
   npm run test -- --run src/etl/__tests__/WordImporter.test.ts
   ```
  </action>
  <verify>
    - `npm run test -- --run src/etl/__tests__/WordImporter.test.ts` runs and all tests FAIL
    - Tests cover conversion, title extraction, encoding, warnings
  </verify>
  <done>Tests written for WordImporter with mammoth mocking. All tests fail (RED phase).</done>
</task>

<task type="auto">
  <name>Task 2: Implement WordImporter to pass tests</name>
  <files>src/etl/importers/WordImporter.ts</files>
  <action>
1. Create WordImporter:

```typescript
/**
 * Word Importer for Isometry ETL
 *
 * Parses .docx files using mammoth.js.
 * Converts Word documents to semantic HTML.
 *
 * Handles binary content via base64 encoding.
 */

import mammoth from 'mammoth';
import { v4 as uuidv4 } from 'uuid';
import { BaseImporter, FileSource } from './BaseImporter';
import { CanonicalNode } from '../types/canonical';
import { generateDeterministicSourceId } from '../id-generation/deterministic';

interface MammothMessage {
  type: string;
  message: string;
}

interface ParsedWord {
  html: string;
  messages: MammothMessage[];
  filename: string;
}

export class WordImporter extends BaseImporter {
  protected async parse(source: FileSource): Promise<unknown> {
    try {
      // Convert content to buffer
      const buffer = this.toBuffer(source.content, source.encoding);

      // Convert DOCX to HTML
      const result = await mammoth.convertToHtml({ buffer });

      return {
        html: result.value,
        messages: result.messages as MammothMessage[],
        filename: source.filename,
      };
    } catch (err) {
      const message = err instanceof Error ? err.message : String(err);
      throw new Error(`Failed to parse ${source.filename}: ${message}`);
    }
  }

  protected async transform(data: unknown): Promise<CanonicalNode[]> {
    const { html, messages, filename } = data as ParsedWord;
    const now = new Date().toISOString();

    // Extract title from first heading
    const title = this.extractTitle(html) || this.extractFilename(filename);

    // Extract summary from first paragraph
    const summary = this.extractSummary(html);

    // Extract text content for word count
    const textContent = this.stripHtml(html);

    const sourceId = generateDeterministicSourceId(
      filename,
      { title, contentHash: this.hashContent(textContent) },
      'word-importer'
    );

    // Collect warnings
    const warnings = messages.filter(m => m.type === 'warning');

    const node: CanonicalNode = {
      id: uuidv4(),
      sourceId,
      source: 'word-importer',
      nodeType: 'note',
      name: title,
      content: html,
      summary,

      // LATCH: Location
      latitude: null,
      longitude: null,
      locationName: null,
      locationAddress: null,

      // LATCH: Time
      createdAt: now,
      modifiedAt: now,
      dueAt: null,
      completedAt: null,
      eventStart: null,
      eventEnd: null,

      // LATCH: Category
      folder: null,
      tags: [],
      status: null,

      // LATCH: Hierarchy
      priority: 0,
      importance: 0,
      sortOrder: 0,

      // Grid
      gridX: 0,
      gridY: 0,

      // Provenance
      sourceUrl: null,
      deletedAt: null,
      version: 1,

      // Extended properties
      properties: {
        originalFormat: 'docx',
        wordCount: textContent.split(/\s+/).filter(Boolean).length,
        ...(warnings.length > 0 ? { conversionWarnings: warnings } : {}),
      },
    };

    return [node];
  }

  private toBuffer(content: string, encoding?: 'utf8' | 'base64'): Buffer {
    if (encoding === 'base64') {
      return Buffer.from(content, 'base64');
    }
    return Buffer.from(content);
  }

  private extractTitle(html: string): string | null {
    // Match first H1, H2, or H3
    const headingMatch = html.match(/<h[123][^>]*>([^<]+)<\/h[123]>/i);
    return headingMatch ? headingMatch[1].trim() : null;
  }

  private extractSummary(html: string): string | null {
    // Find first paragraph after heading (or first paragraph)
    const paragraphs = html.match(/<p[^>]*>([^<]+)<\/p>/gi);
    if (!paragraphs || paragraphs.length === 0) return null;

    // Get first paragraph content
    const firstP = paragraphs[0].replace(/<[^>]+>/g, '').trim();
    return firstP.slice(0, 200) || null;
  }

  private stripHtml(html: string): string {
    return html.replace(/<[^>]+>/g, ' ').replace(/\s+/g, ' ').trim();
  }

  private extractFilename(filepath: string): string {
    const parts = filepath.split(/[/\\]/);
    const filename = parts[parts.length - 1] || 'untitled';
    return filename.replace(/\.docx?$/i, '');
  }

  private hashContent(text: string): string {
    let hash = 0;
    for (let i = 0; i < Math.min(text.length, 1000); i++) {
      hash = ((hash << 5) - hash) + text.charCodeAt(i);
      hash = hash & hash;
    }
    return Math.abs(hash).toString(16);
  }
}
```

2. Run tests:
   ```bash
   npm run test -- --run src/etl/__tests__/WordImporter.test.ts
   ```

3. Run typecheck:
   ```bash
   npm run check:types
   ```
  </action>
  <verify>
    - `npm run test -- --run src/etl/__tests__/WordImporter.test.ts` passes all tests (GREEN)
    - `npm run check:types` has zero errors
  </verify>
  <done>WordImporter implemented and all tests pass.</done>
</task>

<task type="auto">
  <name>Task 3: Add integration test</name>
  <files>src/etl/__tests__/WordImporter.test.ts</files>
  <action>
1. Add integration test:

```typescript
describe('WordImporter integration', () => {
  it('should work with ImportCoordinator', async () => {
    vi.mocked(mammoth.convertToHtml).mockResolvedValue({
      value: '<h1>Integration Test</h1><p>Content</p>',
      messages: [],
    });

    const { ImportCoordinator } = await import('../coordinator/ImportCoordinator');
    const coordinator = new ImportCoordinator();
    coordinator.registerImporter(['.docx'], new WordImporter());

    const nodes = await coordinator.importFile({
      filename: 'integration.docx',
      content: 'base64content',
      encoding: 'base64',
    });

    expect(nodes).toHaveLength(1);
    expect(nodes[0].name).toBe('Integration Test');
    expect(() => CanonicalNodeSchema.parse(nodes[0])).not.toThrow();
  });
});
```

2. Run full test suite:
   ```bash
   npm run test -- --run
   ```
  </action>
  <verify>
    - All WordImporter tests pass
    - Integration with ImportCoordinator works
  </verify>
  <done>WordImporter complete with integration test.</done>
</task>

</tasks>

<verification>
1. `npm run test -- --run src/etl/__tests__/WordImporter.test.ts` - all tests pass
2. `npm run check:types` - zero TypeScript errors
3. DOCX converted to semantic HTML via mammoth
4. Title extracted from first heading
5. Binary/base64 content handled correctly
</verification>

<success_criteria>
1. WordImporter parses DOCX using mammoth
2. Documents converted to semantic HTML
3. Title extracted from first heading
4. Base64 encoding handled for binary content
5. Conversion warnings captured in properties
6. All tests pass with TDD cycle
</success_criteria>

<output>
After completion, create `.planning/phases/69-file-importers/69-05-SUMMARY.md`
</output>
