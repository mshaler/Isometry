---
phase: 69-file-importers
plan: 03
type: tdd
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/etl/importers/CsvImporter.ts
  - src/etl/__tests__/CsvImporter.test.ts
autonomous: true

must_haves:
  truths:
    - "CsvImporter produces one node per row"
    - "Header row maps to LATCH fields with flexible column detection"
    - "CSV edge cases handled (quoted commas, multiline fields)"
    - "Empty rows skipped, parse errors reported clearly"
  artifacts:
    - path: "src/etl/importers/CsvImporter.ts"
      provides: "CSV importer extending BaseImporter"
      exports: ["CsvImporter"]
    - path: "src/etl/__tests__/CsvImporter.test.ts"
      provides: "TDD tests for CSV import"
      min_lines: 80
  key_links:
    - from: "src/etl/importers/CsvImporter.ts"
      to: "src/etl/importers/BaseImporter.ts"
      via: "extends BaseImporter"
      pattern: "extends BaseImporter"
    - from: "src/etl/importers/CsvImporter.ts"
      to: "papaparse"
      via: "Papa.parse"
      pattern: "Papa\\.parse"
---

<objective>
Implement CsvImporter for .csv and .tsv file import using TDD.

Purpose: CSV is common for tabular data exports. PapaParse handles RFC 4180 edge cases (quoted fields, multiline, etc.) that naive string splitting misses. One node per row demonstrates multi-node pattern.

Output: Working CsvImporter using PapaParse with intelligent column-to-LATCH mapping.
</objective>

<execution_context>
@/Users/mshaler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mshaler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/68-import-coordinator/68-01-SUMMARY.md

# Foundation from Phase 68
@src/etl/importers/BaseImporter.ts
@src/etl/types/canonical.ts
@src/etl/id-generation/deterministic.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install papaparse and create failing tests</name>
  <files>
    package.json
    src/etl/__tests__/CsvImporter.test.ts
  </files>
  <action>
1. Install papaparse:
   ```bash
   npm install papaparse
   npm install -D @types/papaparse
   ```

2. Create CsvImporter.test.ts with failing tests:
```typescript
import { CsvImporter } from '../importers/CsvImporter';
import { CanonicalNodeSchema } from '../types/canonical';
import { FileSource } from '../importers/BaseImporter';

describe('CsvImporter', () => {
  let importer: CsvImporter;

  beforeEach(() => {
    importer = new CsvImporter();
  });

  describe('basic parsing', () => {
    it('should import CSV with header row', async () => {
      const csv = `name,status,priority
Task 1,todo,high
Task 2,done,low
Task 3,in-progress,medium`;

      const nodes = await importer.import({
        filename: 'tasks.csv',
        content: csv,
      });

      expect(nodes).toHaveLength(3);
      expect(nodes[0].name).toBe('Task 1');
      expect(nodes[0].status).toBe('todo');
      expect(nodes[0].priority).toBe(5); // 'high' -> 5
      expect(nodes[1].name).toBe('Task 2');
      expect(nodes[2].name).toBe('Task 3');
    });

    it('should handle TSV (tab-separated)', async () => {
      const tsv = `title\tfolder\ttags
Note 1\tWork\turgent,important
Note 2\tPersonal\tlow-priority`;

      const nodes = await importer.import({
        filename: 'notes.tsv',
        content: tsv,
      });

      expect(nodes).toHaveLength(2);
      expect(nodes[0].name).toBe('Note 1');
      expect(nodes[0].folder).toBe('Work');
      expect(nodes[0].tags).toEqual(['urgent', 'important']);
    });
  });

  describe('edge cases', () => {
    it('should handle quoted fields with commas', async () => {
      const csv = `name,description
"Task, with comma","Description with ""quotes"" inside"`;

      const nodes = await importer.import({
        filename: 'quoted.csv',
        content: csv,
      });

      expect(nodes[0].name).toBe('Task, with comma');
      expect(nodes[0].content).toContain('Description with "quotes" inside');
    });

    it('should skip empty rows', async () => {
      const csv = `name
Row 1

Row 2

`;

      const nodes = await importer.import({
        filename: 'sparse.csv',
        content: csv,
      });

      expect(nodes).toHaveLength(2);
    });

    it('should return empty array for empty CSV', async () => {
      const nodes = await importer.import({
        filename: 'empty.csv',
        content: '',
      });

      expect(nodes).toHaveLength(0);
    });

    it('should return empty array for header-only CSV', async () => {
      const csv = `name,status,priority`;

      const nodes = await importer.import({
        filename: 'header-only.csv',
        content: csv,
      });

      expect(nodes).toHaveLength(0);
    });
  });

  describe('LATCH column mapping', () => {
    it('should detect name from various column headers', async () => {
      const testCases = [
        { header: 'name', expected: 'Test Value' },
        { header: 'title', expected: 'Test Value' },
        { header: 'task', expected: 'Test Value' },
        { header: 'subject', expected: 'Test Value' },
      ];

      for (const { header, expected } of testCases) {
        const csv = `${header}\n${expected}`;
        const nodes = await importer.import({
          filename: 'test.csv',
          content: csv,
        });
        expect(nodes[0].name).toBe(expected);
      }
    });

    it('should detect date columns', async () => {
      const csv = `name,created,due_date,modified
Task 1,2024-01-15T10:00:00Z,2024-02-01T00:00:00Z,2024-01-20T12:00:00Z`;

      const nodes = await importer.import({
        filename: 'dates.csv',
        content: csv,
      });

      expect(nodes[0].createdAt).toBe('2024-01-15T10:00:00Z');
      expect(nodes[0].dueAt).toBe('2024-02-01T00:00:00Z');
      expect(nodes[0].modifiedAt).toBe('2024-01-20T12:00:00Z');
    });

    it('should detect category columns', async () => {
      const csv = `name,folder,category,tags
Task 1,Work,,urgent;important`;

      const nodes = await importer.import({
        filename: 'categories.csv',
        content: csv,
      });

      expect(nodes[0].folder).toBe('Work');
      expect(nodes[0].tags).toEqual(['urgent', 'important']);
    });
  });

  describe('deterministic IDs', () => {
    it('should generate unique sourceId per row', async () => {
      const csv = `name
Row 1
Row 2`;

      const nodes = await importer.import({
        filename: 'rows.csv',
        content: csv,
      });

      expect(nodes[0].sourceId).not.toBe(nodes[1].sourceId);
      expect(nodes[0].sourceId).toMatch(/^csv-importer-/);
    });

    it('should generate consistent sourceId on reimport', async () => {
      const csv = `name\nTest Row`;
      const source: FileSource = { filename: 'consistent.csv', content: csv };

      const nodes1 = await importer.import(source);
      const nodes2 = await importer.import(source);

      expect(nodes1[0].sourceId).toBe(nodes2[0].sourceId);
    });
  });

  describe('validation', () => {
    it('should produce valid CanonicalNode', async () => {
      const csv = `name,created
Valid Task,2024-01-15T10:00:00Z`;

      const nodes = await importer.import({
        filename: 'valid.csv',
        content: csv,
      });

      expect(() => CanonicalNodeSchema.parse(nodes[0])).not.toThrow();
    });
  });
});
```

3. Run tests to verify they fail:
   ```bash
   npm run test -- --run src/etl/__tests__/CsvImporter.test.ts
   ```
  </action>
  <verify>
    - papaparse installed (check package.json)
    - `npm run test -- --run src/etl/__tests__/CsvImporter.test.ts` runs and all tests FAIL
  </verify>
  <done>Tests written for CsvImporter covering parsing, edge cases, LATCH mapping. All tests fail (RED phase).</done>
</task>

<task type="auto">
  <name>Task 2: Implement CsvImporter to pass tests</name>
  <files>src/etl/importers/CsvImporter.ts</files>
  <action>
1. Create CsvImporter:

```typescript
/**
 * CSV Importer for Isometry ETL
 *
 * Parses .csv and .tsv files using PapaParse.
 * One node per row, header row maps to fields.
 *
 * LATCH mapping with intelligent column detection.
 */

import Papa from 'papaparse';
import { v4 as uuidv4 } from 'uuid';
import { BaseImporter, FileSource } from './BaseImporter';
import { CanonicalNode } from '../types/canonical';
import { generateDeterministicSourceId } from '../id-generation/deterministic';

interface ParsedCsv {
  rows: Record<string, string>[];
  filename: string;
}

export class CsvImporter extends BaseImporter {
  protected async parse(source: FileSource): Promise<unknown> {
    return new Promise((resolve, reject) => {
      Papa.parse<Record<string, string>>(source.content, {
        header: true,
        skipEmptyLines: true,
        transformHeader: (header) => header.trim(),
        complete: (results) => {
          if (results.errors.length > 0) {
            // Filter out recoverable errors
            const fatalErrors = results.errors.filter(e => e.type === 'Quotes');
            if (fatalErrors.length > 0) {
              reject(new Error(
                `CSV parse errors in ${source.filename}: ${JSON.stringify(fatalErrors)}`
              ));
              return;
            }
          }
          resolve({ rows: results.data, filename: source.filename });
        },
        error: (err) => reject(new Error(`CSV parse failed: ${err.message}`)),
      });
    });
  }

  protected async transform(data: unknown): Promise<CanonicalNode[]> {
    const { rows, filename } = data as ParsedCsv;
    const now = new Date().toISOString();

    return rows.map((row, index) => this.rowToNode(row, filename, index, now));
  }

  private rowToNode(
    row: Record<string, string>,
    filename: string,
    index: number,
    now: string
  ): CanonicalNode {
    const sourceId = generateDeterministicSourceId(
      `${filename}:row:${index}`,
      row,
      'csv-importer'
    );

    return {
      id: uuidv4(),
      sourceId,
      source: 'csv-importer',
      nodeType: detectValue(row, ['type', 'node_type', 'nodeType']) || 'note',
      name: detectName(row, index),
      content: JSON.stringify(row, null, 2),
      summary: detectValue(row, ['summary', 'description', 'notes'])?.slice(0, 200) || null,

      // LATCH: Location
      latitude: parseFloat(detectValue(row, ['latitude', 'lat']) || '') || null,
      longitude: parseFloat(detectValue(row, ['longitude', 'lng', 'lon']) || '') || null,
      locationName: detectValue(row, ['location_name', 'locationName', 'place']) || null,
      locationAddress: detectValue(row, ['address', 'location', 'locationAddress']) || null,

      // LATCH: Time
      createdAt: detectDate(row, ['created', 'createdAt', 'created_at', 'date', 'timestamp']) || now,
      modifiedAt: detectDate(row, ['modified', 'modifiedAt', 'modified_at', 'updated']) || now,
      dueAt: detectDate(row, ['due', 'dueAt', 'due_at', 'due_date', 'deadline']) || null,
      completedAt: detectDate(row, ['completed', 'completedAt', 'completed_at']) || null,
      eventStart: detectDate(row, ['start', 'eventStart', 'start_date']) || null,
      eventEnd: detectDate(row, ['end', 'eventEnd', 'end_date']) || null,

      // LATCH: Category
      folder: detectValue(row, ['folder', 'category', 'group', 'project']) || null,
      tags: detectTags(row),
      status: detectValue(row, ['status', 'state']) || null,

      // LATCH: Hierarchy
      priority: detectPriority(row),
      importance: parseInt(detectValue(row, ['importance']) || '0', 10) || 0,
      sortOrder: parseInt(detectValue(row, ['sort_order', 'sortOrder', 'order']) || '0', 10),

      // Grid
      gridX: 0,
      gridY: 0,

      // Provenance
      sourceUrl: detectValue(row, ['url', 'link', 'source_url']) || null,
      deletedAt: null,
      version: 1,

      // Store column headers for reference
      properties: {
        originalFormat: 'csv',
        rowIndex: index,
        columns: Object.keys(row),
      },
    };
  }
}

// Helper functions
function detectValue(row: Record<string, string>, keys: string[]): string | null {
  for (const key of keys) {
    // Case-insensitive matching
    const matchedKey = Object.keys(row).find(
      k => k.toLowerCase() === key.toLowerCase()
    );
    if (matchedKey && row[matchedKey]?.trim()) {
      return row[matchedKey].trim();
    }
  }
  return null;
}

function detectName(row: Record<string, string>, index: number): string {
  const nameKeys = ['name', 'title', 'task', 'subject', 'description', 'item'];
  const name = detectValue(row, nameKeys);
  if (name) return name;

  // Fallback to first non-empty column value
  for (const value of Object.values(row)) {
    if (value?.trim()) return value.trim();
  }

  return `Row ${index + 1}`;
}

function detectDate(row: Record<string, string>, keys: string[]): string | null {
  const value = detectValue(row, keys);
  if (!value) return null;

  // Already ISO format
  if (/^\d{4}-\d{2}-\d{2}T/.test(value)) return value;

  // Try to parse
  const parsed = new Date(value);
  if (!isNaN(parsed.getTime())) return parsed.toISOString();

  return null;
}

function detectTags(row: Record<string, string>): string[] {
  const tagValue = detectValue(row, ['tags', 'labels', 'categories', 'keywords']);
  if (!tagValue) return [];

  // Split by comma or semicolon
  return tagValue.split(/[,;]/).map(t => t.trim()).filter(Boolean);
}

function detectPriority(row: Record<string, string>): number {
  const p = detectValue(row, ['priority']);
  if (!p) return 0;

  // Numeric
  const num = parseInt(p, 10);
  if (!isNaN(num)) return Math.min(5, Math.max(0, num));

  // String values
  const lower = p.toLowerCase();
  if (lower === 'high' || lower === 'urgent' || lower === 'critical') return 5;
  if (lower === 'medium' || lower === 'normal') return 3;
  if (lower === 'low') return 1;

  return 0;
}
```

2. Run tests:
   ```bash
   npm run test -- --run src/etl/__tests__/CsvImporter.test.ts
   ```

3. Run typecheck:
   ```bash
   npm run check:types
   ```
  </action>
  <verify>
    - `npm run test -- --run src/etl/__tests__/CsvImporter.test.ts` passes all tests (GREEN)
    - `npm run check:types` has zero errors
  </verify>
  <done>CsvImporter implemented and all tests pass.</done>
</task>

<task type="auto">
  <name>Task 3: Add integration test</name>
  <files>src/etl/__tests__/CsvImporter.test.ts</files>
  <action>
1. Add integration test:

```typescript
describe('CsvImporter integration', () => {
  it('should work with ImportCoordinator', async () => {
    const { ImportCoordinator } = await import('../coordinator/ImportCoordinator');
    const coordinator = new ImportCoordinator();
    coordinator.registerImporter(['.csv', '.tsv'], new CsvImporter());

    const csv = `name,status
Task 1,todo
Task 2,done`;

    const nodes = await coordinator.importFile({
      filename: 'integration.csv',
      content: csv,
    });

    expect(nodes).toHaveLength(2);
    nodes.forEach(node => {
      expect(() => CanonicalNodeSchema.parse(node)).not.toThrow();
    });
  });
});
```

2. Run full test suite:
   ```bash
   npm run test -- --run
   ```
  </action>
  <verify>
    - All CsvImporter tests pass
    - Integration with ImportCoordinator works
  </verify>
  <done>CsvImporter complete with integration test.</done>
</task>

</tasks>

<verification>
1. `npm run test -- --run src/etl/__tests__/CsvImporter.test.ts` - all tests pass
2. `npm run check:types` - zero TypeScript errors
3. PapaParse handles edge cases (quoted fields, multiline)
4. One node per row with intelligent column mapping
</verification>

<success_criteria>
1. CsvImporter parses CSV/TSV using PapaParse
2. One node per data row
3. Header row used for column names
4. LATCH fields mapped with flexible column detection
5. Edge cases handled (quotes, commas, empty rows)
6. All tests pass with TDD cycle
</success_criteria>

<output>
After completion, create `.planning/phases/69-file-importers/69-03-SUMMARY.md`
</output>
