---
phase: 30
plan: 30-02
type: auto
autonomous: true
wave: 2
title: "Advanced Web Clipping with Image Caching and Ethical Crawling"
estimated_duration: "60-75 minutes"
dependencies: ["30-01"]
verification: true
---

# Wave 2: Advanced Web Clipping with Image Caching and Ethical Crawling

**Goal:** Enhance web clipping with image downloading/caching, robots.txt compliance, rate limiting, and privacy-respecting design.

**Wave:** 2 of 4 (extraction → caching → UI → integration)
**Type:** autonomous
**Duration:** 60-75 minutes
**Dependencies:** Wave 1 (30-01) - Core web clipping infrastructure

## Context Files

@.planning/phases/30-html-web-clipper-pipeline/30-01-PLAN.md - Previous wave implementation
@native/Sources/Isometry/Database/ - Database patterns for caching
@native/Sources/Isometry/Sync/ - CloudKit sync patterns for reference

## Objective

Extend web clipping capabilities with:
- Image downloading and SHA-256 deduplication caching
- Ethical crawling with robots.txt compliance and rate limiting
- Privacy-respecting design with no persistent tracking
- Advanced content extraction for dynamic websites

## Tasks

### Task 1: Image Caching System
**Type:** auto
**Description:** Implement image downloading and caching with SHA-256 deduplication to avoid storing duplicate images.

**Implementation:**
- Create ImageCacheActor for managing image downloads
- Download images referenced in clipped content
- Calculate SHA-256 hashes for deduplication
- Store images in Documents/WebClipImages/ directory
- Update content HTML to reference local cached images
- Handle various image formats (PNG, JPG, GIF, WebP, SVG)

**Files to create:**
- `native/Sources/Isometry/WebClipper/ImageCacheActor.swift`
- `native/Sources/Isometry/WebClipper/ImageDeduplication.swift`

**Verification:** Images from web pages are cached locally with deduplication working

### Task 2: Ethical Crawling Infrastructure
**Type:** auto
**Description:** Implement robots.txt compliance, rate limiting, and respectful crawling behavior.

**Implementation:**
- Create RobotsTxtParser to check crawling permissions
- Implement rate limiting (configurable delay between requests)
- Add User-Agent header with proper identification
- Respect Crawl-Delay directive from robots.txt
- Handle HTTP status codes appropriately (301, 302, 404, 429)
- Implement exponential backoff for failures

**Files to create:**
- `native/Sources/Isometry/WebClipper/EthicalCrawler.swift`
- `native/Sources/Isometry/WebClipper/RobotsTxtParser.swift`
- `native/Sources/Isometry/WebClipper/RateLimiter.swift`

**Verification:** Web clipper respects robots.txt and implements proper rate limiting

### Task 3: Privacy Protection System
**Type:** auto
**Description:** Ensure privacy-respecting design with no cookie/localStorage persistence or tracking.

**Implementation:**
- Configure WKWebView with ephemeral session
- Disable JavaScript tracking and analytics
- Strip tracking parameters from URLs
- No persistent cookies or localStorage
- User-Agent rotation (if needed)
- Remove referrer headers for privacy

**Files to create:**
- `native/Sources/Isometry/WebClipper/PrivacyEngine.swift`
- `native/Sources/Isometry/WebClipper/TrackingProtection.swift`

**Verification:** Web clipping operates without leaving tracking footprints

### Task 4: Advanced Content Extraction
**Type:** auto
**Description:** Handle dynamic websites, JavaScript-rendered content, and complex page structures.

**Implementation:**
- Add JavaScript execution capability for dynamic content
- Handle single-page applications (SPA) content loading
- Extract content from shadow DOM if present
- Support pagination detection and handling
- Handle lazy-loaded images and content
- Implement timeout handling for slow-loading pages

**Verification:** Dynamic websites and JavaScript-heavy pages extract content correctly

## Verification Criteria

1. **Image caching works efficiently**
   - Images downloaded and stored locally
   - SHA-256 deduplication prevents duplicates
   - HTML content updated with local image references
   - Various image formats supported
   - Cache size management implemented

2. **Ethical crawling compliance established**
   - robots.txt checked before crawling
   - Rate limiting enforced (configurable delays)
   - Proper User-Agent header sent
   - HTTP status codes handled appropriately
   - Exponential backoff for failures

3. **Privacy protection comprehensive**
   - No cookies or localStorage persistence
   - Tracking parameters stripped from URLs
   - Ephemeral browsing sessions used
   - No referrer information leaked
   - Analytics scripts blocked

4. **Advanced extraction handles dynamic content**
   - JavaScript-rendered content extracted
   - Single-page applications supported
   - Lazy-loaded content handled
   - Timeout protection for slow sites
   - Complex page structures parsed

## Success Criteria

- Images from clipped pages cached locally with deduplication
- robots.txt compliance prevents unauthorized crawling
- Rate limiting ensures respectful server behavior
- Privacy protection prevents tracking exposure
- Dynamic content extraction works on modern websites

## Done Criteria

- [ ] Image caching system operational with SHA-256 deduplication
- [ ] Robots.txt compliance and rate limiting implemented
- [ ] Privacy protection prevents tracking and cookie persistence
- [ ] Advanced content extraction handles JavaScript-heavy sites
- [ ] Performance remains acceptable with new features