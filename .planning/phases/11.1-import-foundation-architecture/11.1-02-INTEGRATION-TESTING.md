# Integration Testing Protocols - Phase 11.1-02
# v2.5 Advanced Import Systems - End-to-End Workflow Validation

**Generated:** 2026-01-27
**Phase:** 11.1-02 Data Quality Framework & Integration Testing
**Milestone:** v2.5 Advanced Import Systems

## Executive Summary

This integration testing protocol framework establishes comprehensive end-to-end workflow validation for all import systems. The framework provides systematic testing procedures for cross-system integration, multi-format import operations, performance validation under enterprise conditions, and comprehensive error handling scenarios.

**Testing Framework Coverage:**
- 5 distinct integration testing categories
- 25+ comprehensive test scenarios
- Enterprise-scale performance testing: 100MB+ files, million-record databases
- Complete workflow validation from file selection to database integration

## 1. Cross-System Integration Tests

### 1.1 Office to Isometry Graph Transformation Testing

#### Document Structure → Node Hierarchy Validation
1. **Excel Worksheet to Node Hierarchy Testing**
   ```
   Test Scenario: Multi-worksheet Excel file import
   Source: Excel file with 5 worksheets, hierarchical data
   Expected Outcome:
     - Root node representing the workbook
     - Child nodes for each worksheet
     - Data nodes for significant content areas
     - Proper parent-child relationships preserved

   Validation Procedures:
     1. Verify workbook-level metadata node creation
     2. Confirm worksheet hierarchy preservation
     3. Check data content mapping accuracy
     4. Validate cross-sheet reference documentation
   ```

2. **Word Document Structure to Graph Integration Testing**
   ```
   Test Scenario: Complex Word document with headings and sections
   Source: Corporate document with 3-level heading hierarchy, tables, lists
   Expected Outcome:
     - Document root node with metadata
     - Section nodes following heading structure
     - Content nodes for paragraphs, tables, and lists
     - Hierarchical relationships matching document structure

   Validation Procedures:
     1. Check heading hierarchy preservation in node structure
     2. Verify table content extraction and node creation
     3. Validate list structure mapping
     4. Confirm document metadata integration
   ```

3. **Cross-Reference and Link Preservation Testing**
   ```
   Test Scenario: Office documents with internal and external references
   Source: Excel with named ranges, Word with cross-references
   Expected Outcome:
     - Reference relationships captured as node connections
     - External link documentation in node metadata
     - Named range definitions preserved in node properties

   Validation Procedures:
     1. Verify internal reference mapping accuracy
     2. Check external link preservation
     3. Validate named range documentation
     4. Confirm cross-document reference handling
   ```

#### Integration Test Success Criteria
- Document structure preservation: ≥90% hierarchical accuracy
- Content node creation: 100% significant content captured
- Reference mapping: ≥85% cross-reference preservation
- Metadata integration: ≥95% document properties captured

### 1.2 SQLite to Isometry Graph Mapping Validation

#### Relational Data → Graph Edge Transformation Testing
1. **Foreign Key Relationship Mapping Testing**
   ```
   Test Scenario: SQLite database with complex foreign key relationships
   Source: CRM database with customers, orders, products tables
   Expected Outcome:
     - Customer nodes with order relationship edges
     - Product nodes connected to order items
     - Proper edge directionality and labeling
     - Relationship metadata preservation

   Validation Procedures:
     1. Verify all foreign key relationships converted to edges
     2. Check edge direction and labeling accuracy
     3. Validate relationship metadata preservation
     4. Confirm referential integrity maintenance
   ```

2. **Table Structure to Node Pattern Validation**
   ```
   Test Scenario: Database with various table patterns (lookup, junction, data)
   Source: E-commerce database with products, categories, product_categories
   Expected Outcome:
     - Entity tables → primary nodes
     - Lookup tables → attribute enrichment
     - Junction tables → many-to-many edges
     - Proper categorization and tagging

   Validation Procedures:
     1. Check entity table conversion accuracy
     2. Verify lookup table data integration
     3. Validate junction table relationship handling
     4. Confirm proper node categorization
   ```

3. **Data Type Conversion and Integrity Validation**
   ```
   Test Scenario: Database with diverse data types and constraints
   Source: Database with timestamps, BLOB data, JSON fields, constraints
   Expected Outcome:
     - Accurate data type conversion
     - Constraint information preservation
     - Special data type handling (JSON, BLOB)
     - No data loss during transformation

   Validation Procedures:
     1. Verify data type conversion accuracy
     2. Check constraint preservation
     3. Validate special data type handling
     4. Confirm zero data loss in transformation
   ```

#### Integration Test Success Criteria
- Relationship mapping accuracy: ≥95% foreign key conversion
- Data type preservation: ≥98% accurate conversion
- Constraint documentation: ≥90% constraint information captured
- Zero data loss: 100% data integrity maintained

### 1.3 Apple Ecosystem to Real-Time Sync Integration Testing

#### Bi-Directional Synchronization Validation
1. **Notes App Real-Time Sync Testing**
   ```
   Test Scenario: Notes app modification during active sync
   Source: Active Notes database with ongoing changes
   Expected Outcome:
     - Real-time change detection (<5 second latency)
     - Accurate delta synchronization
     - Conflict detection for simultaneous changes
     - Proper sync state management

   Validation Procedures:
     1. Create note in Notes app during sync
     2. Modify existing note content
     3. Delete note and verify sync handling
     4. Test conflict resolution mechanisms
   ```

2. **Multi-App Coordination Testing**
   ```
   Test Scenario: Simultaneous sync from Notes, Reminders, Calendar
   Source: Multiple Apple apps with active data changes
   Expected Outcome:
     - Coordinated sync without resource conflicts
     - Proper prioritization and queue management
     - Independent error handling per app
     - Consistent data state across all sources

   Validation Procedures:
     1. Initiate sync for all enabled apps simultaneously
     2. Monitor resource usage and coordination
     3. Verify independent error handling
     4. Check final data consistency across sources
   ```

3. **Permission and Privacy Compliance Testing**
   ```
   Test Scenario: Sync operations with varying permission states
   Source: Apple apps with different permission configurations
   Expected Outcome:
     - Graceful handling of denied permissions
     - Proper user notification and guidance
     - Partial sync success for accessible data
     - 100% privacy compliance maintained

   Validation Procedures:
     1. Test sync with full permissions granted
     2. Revoke permissions during sync operation
     3. Test partial permission scenarios
     4. Verify privacy compliance maintenance
   ```

#### Integration Test Success Criteria
- Sync latency: <5 seconds for change detection
- Sync accuracy: ≥99% for accessible data
- Privacy compliance: 100% user consent respect
- Resource coordination: ≥90% efficient multi-app handling

## 2. Workflow Integration Tests

### 2.1 Multi-Format Import Workflow Validation

#### Sequential Multi-Format Import Testing
1. **Office → SQLite → Apple Sequential Import**
   ```
   Test Scenario: Import sequence with Office docs, SQLite DB, Apple data
   Source: Corporate Excel file, CRM database, Apple Notes export
   Expected Outcome:
     - Successful processing in sequence
     - No resource conflicts between import types
     - Consistent data quality across all formats
     - Proper cleanup between import operations

   Validation Steps:
     1. Import Excel file with complex data
     2. Import SQLite database with relationships
     3. Import Apple Notes with folder structure
     4. Verify no data corruption or conflicts
     5. Check resource cleanup between operations
   ```

2. **Data Relationship Discovery Across Formats**
   ```
   Test Scenario: Related data across different import sources
   Source: Customer data in Excel, orders in SQLite, notes in Apple Notes
   Expected Outcome:
     - Intelligent relationship detection suggestions
     - Cross-format data correlation opportunities
     - Consistent entity recognition across sources
     - Merge/deduplication recommendations

   Validation Steps:
     1. Import customer list from Excel
     2. Import order database from SQLite
     3. Import customer communication notes
     4. Check for relationship detection accuracy
     5. Verify deduplication suggestions
   ```

3. **Metadata Consistency Across Import Types**
   ```
   Test Scenario: Consistent metadata handling across all formats
   Source: Files with similar metadata (creation date, author, tags)
   Expected Outcome:
     - Consistent metadata extraction patterns
     - Standardized metadata format in Isometry
     - Proper source attribution across formats
     - Metadata-based relationship suggestions

   Validation Steps:
     1. Import files with overlapping metadata
     2. Verify consistent metadata extraction
     3. Check standardized metadata format
     4. Validate source attribution accuracy
   ```

#### Multi-Format Integration Success Criteria
- Sequential import success: ≥95% completion rate
- Resource efficiency: <20% overhead for format switching
- Metadata consistency: ≥90% standardization across formats
- Relationship detection: ≥80% accuracy for obvious relationships

### 2.2 Concurrent Import Operations Testing

#### Simultaneous Multi-File Import Validation
1. **Concurrent Office Document Processing**
   ```
   Test Scenario: 5 large Office documents imported simultaneously
   Source: Multiple Excel and Word files (10-50MB each)
   Expected Outcome:
     - Successful concurrent processing
     - Memory usage within acceptable limits
     - Processing time efficiency vs. sequential
     - No resource starvation or deadlocks

   Performance Targets:
     - Memory usage: <500% total file size
     - Processing time: <150% of sequential time
     - Success rate: ≥95% for all concurrent operations
     - System responsiveness: Maintained throughout
   ```

2. **Mixed Format Concurrent Import Testing**
   ```
   Test Scenario: Office, SQLite, Apple imports running concurrently
   Source: Excel file, SQLite database, Apple Notes sync
   Expected Outcome:
     - Proper resource allocation across import types
     - No interference between different import engines
     - Coordinated progress reporting
     - Graceful handling of resource constraints

   Validation Metrics:
     - Resource coordination: ≥90% efficiency
     - Progress accuracy: ≥95% real-time reporting
     - Error isolation: 100% between import types
     - Completion consistency: ≥95% success rate
   ```

3. **Import Queue Management and Prioritization**
   ```
   Test Scenario: Import queue with priority handling
   Source: Mix of small and large files with different priorities
   Expected Outcome:
     - Proper queue management and ordering
     - Priority-based processing execution
     - Dynamic queue reordering capability
     - Efficient resource utilization

   Queue Management Tests:
     1. Submit multiple imports with different priorities
     2. Monitor queue processing order
     3. Test dynamic priority changes
     4. Verify resource allocation efficiency
   ```

#### Concurrent Operations Success Criteria
- Concurrent processing capability: Support 3+ simultaneous imports
- Resource efficiency: <200% memory usage vs. sequential
- Progress accuracy: ≥95% real-time progress reporting
- Error isolation: 100% between concurrent operations

### 2.3 Import Pipeline Integration Testing

#### End-to-End Pipeline Validation
1. **File Selection to Database Integration Workflow**
   ```
   Test Scenario: Complete user workflow from file selection to data availability
   Source: Mixed file types with various sizes and complexities
   Expected Outcome:
     - Seamless user experience throughout workflow
     - Accurate progress reporting at each stage
     - Proper error handling and user guidance
     - Data immediately available after completion

   Workflow Steps:
     1. File selection and validation
     2. Format detection and importer assignment
     3. Processing with progress updates
     4. Quality validation and reporting
     5. Database integration and availability
     6. User notification and next steps
   ```

2. **Import Metadata and Audit Trail Validation**
   ```
   Test Scenario: Complete audit trail creation for all import operations
   Source: Various file types with detailed tracking requirements
   Expected Outcome:
     - Complete import operation documentation
     - Source attribution for all imported data
     - Processing statistics and quality metrics
     - User action and decision recording

   Audit Trail Requirements:
     1. Source file information and checksums
     2. Processing timestamps and duration
     3. Quality scores and validation results
     4. User decisions and configurations
     5. Error occurrences and resolution actions
   ```

3. **Import Result Aggregation and Reporting**
   ```
   Test Scenario: Comprehensive reporting for complex import operations
   Source: Batch import with multiple files and formats
   Expected Outcome:
     - Consolidated import results reporting
     - Quality metrics aggregation across operations
     - Performance analytics and insights
     - Actionable recommendations for improvement

   Reporting Components:
     1. Overall success rate and quality scores
     2. Performance metrics and bottleneck identification
     3. Data quality insights and improvement suggestions
     4. User experience feedback and optimization opportunities
   ```

#### Pipeline Integration Success Criteria
- End-to-end workflow completion: ≥95% success rate
- Audit trail completeness: 100% operation documentation
- Result aggregation accuracy: ≥98% metric calculation
- User experience quality: ≥90% workflow satisfaction

## 3. Performance Integration Tests

### 3.1 Large Dataset Handling Validation

#### Office Document Stress Testing
1. **Large Excel File Processing (100MB+)**
   ```
   Test Scenario: Excel files with 1M+ cells and complex formulas
   Source: Corporate financial models, data analysis workbooks
   Expected Outcome:
     - Successful processing without memory exhaustion
     - Processing time within acceptable limits
     - Maintain system responsiveness
     - Accurate data extraction despite size

   Performance Targets:
     - Memory usage: <5x file size peak usage
     - Processing speed: ≥500KB/sec sustained rate
     - System impact: <40% CPU utilization
     - Success rate: ≥90% for well-formed large files
   ```

2. **Complex Word Document Processing**
   ```
   Test Scenario: Large Word documents (50MB+) with rich content
   Source: Technical manuals, legal documents, academic papers
   Expected Outcome:
     - Complete content extraction without timeout
     - Table and formatting preservation
     - Reasonable processing performance
     - Memory management within limits

   Performance Validation:
     1. Monitor memory usage throughout processing
     2. Measure processing speed and bottlenecks
     3. Verify content extraction completeness
     4. Check system stability during processing
   ```

#### SQLite Database Stress Testing
1. **Million-Record Database Import**
   ```
   Test Scenario: SQLite databases with 1M+ records across multiple tables
   Source: Enterprise CRM, analytics, or historical data systems
   Expected Outcome:
     - Complete data migration without loss
     - Reasonable processing time with progress updates
     - Memory usage scaling linearly with data size
     - System stability throughout operation

   Stress Test Metrics:
     - Processing rate: ≥10,000 records/sec
     - Memory scaling: Linear with dataset size
     - Progress accuracy: ≥95% reporting fidelity
     - System stability: No crashes or freezes
   ```

2. **Complex Relationship Database Testing**
   ```
   Test Scenario: Database with extensive foreign key relationships
   Source: E-commerce platform database with 20+ related tables
   Expected Outcome:
     - All relationships properly mapped and preserved
     - Efficient relationship processing
     - No referential integrity violations
     - Complete graph structure creation

   Relationship Processing Tests:
     1. Verify all foreign keys detected and processed
     2. Check relationship mapping accuracy
     3. Validate graph structure integrity
     4. Monitor performance during relationship processing
   ```

#### Apple Ecosystem Scale Testing
1. **Large Notes Collection Import**
   ```
   Test Scenario: Apple Notes with 10,000+ notes across folders
   Source: Long-term Notes user with extensive folder structure
   Expected Outcome:
     - Complete notes collection import
     - Folder hierarchy preservation
     - Efficient batch processing
     - Real-time progress reporting

   Scale Testing Validation:
     1. Monitor processing rate for large collections
     2. Check memory usage scaling
     3. Verify folder structure accuracy
     4. Validate content preservation quality
   ```

### 3.2 Memory Stress Testing

#### Concurrent Large File Processing
1. **Multiple Large File Memory Management**
   ```
   Test Scenario: 3+ large files (50MB+ each) processed concurrently
   Source: Mix of Excel, Word, SQLite files
   Expected Outcome:
     - Memory usage within system limits
     - No memory leaks or excessive growth
     - Proper cleanup after processing completion
     - System stability maintained

   Memory Management Validation:
     1. Monitor peak memory usage
     2. Track memory cleanup efficiency
     3. Check for memory leaks
     4. Verify system stability under load
   ```

2. **Extended Operation Memory Stability**
   ```
   Test Scenario: Continuous import operations over 1-hour period
   Source: Repeated import cycles with various file types
   Expected Outcome:
     - Stable memory usage over time
     - No memory leak accumulation
     - Consistent performance throughout session
     - Proper resource cleanup between operations

   Stability Metrics:
     - Memory growth: <5% over baseline per hour
     - Performance degradation: <10% over session
     - Resource cleanup: 100% between operations
     - System responsiveness: Maintained throughout
   ```

### 3.3 Real-Time Responsiveness Testing

#### UI Updates During Import Operations
1. **Progress Reporting Accuracy and Responsiveness**
   ```
   Test Scenario: Real-time progress updates during large import operations
   Source: Large files with measurable processing stages
   Expected Outcome:
     - Progress updates ≤1 second latency
     - Accurate progress percentage reporting
     - Responsive UI throughout operation
     - Clear stage information display

   Responsiveness Tests:
     1. Measure progress update frequency and accuracy
     2. Test UI responsiveness during peak processing
     3. Verify stage information accuracy
     4. Check overall user experience quality
   ```

2. **Cancel and Resume Operation Testing**
   ```
   Test Scenario: User-initiated cancellation and operation resumption
   Source: Long-running import operations
   Expected Outcome:
     - Immediate response to cancel requests
     - Clean operation termination and cleanup
     - Resume capability where applicable
     - Proper state management throughout

   Cancel/Resume Validation:
     1. Test immediate cancel response
     2. Verify cleanup after cancellation
     3. Check resume capability
     4. Validate state consistency
   ```

## 4. Error Handling Integration Tests

### 4.1 Partial Failure Recovery Testing

#### Import Job Interruption and Resumption
1. **Network Interruption Simulation**
   ```
   Test Scenario: Network connectivity loss during Apple ecosystem sync
   Source: Active Apple Notes sync with simulated network failure
   Expected Outcome:
     - Graceful handling of network interruption
     - Proper error detection and reporting
     - Automatic retry mechanisms where appropriate
     - State preservation for resumption

   Interruption Recovery Tests:
     1. Simulate network failure during sync
     2. Test error detection and user notification
     3. Verify automatic retry behavior
     4. Check state preservation and resumption
   ```

2. **Disk Space Exhaustion Handling**
   ```
   Test Scenario: Disk space exhaustion during large file processing
   Source: Large Office documents with limited available storage
   Expected Outcome:
     - Early detection of space constraints
     - Graceful operation termination
     - Clear user guidance for resolution
     - Proper cleanup of partial processing

   Space Exhaustion Tests:
     1. Monitor disk space during processing
     2. Test early warning systems
     3. Verify graceful failure handling
     4. Check cleanup effectiveness
   ```

3. **Memory Exhaustion Recovery**
   ```
   Test Scenario: System memory limits during concurrent operations
   Source: Multiple large files pushing system memory limits
   Expected Outcome:
     - Intelligent operation throttling
     - Memory pressure detection
     - Graceful degradation of performance
     - Stable operation within limits

   Memory Recovery Validation:
     1. Test memory pressure detection
     2. Verify operation throttling mechanisms
     3. Check graceful performance degradation
     4. Validate system stability maintenance
   ```

### 4.2 Corrupt Data Handling Testing

#### Malformed File Processing
1. **Corrupted Office Document Handling**
   ```
   Test Scenario: Office files with ZIP corruption and XML malformation
   Source: Deliberately corrupted Excel and Word files
   Expected Outcome:
     - Proper corruption detection
     - Meaningful error reporting to user
     - Partial data recovery where possible
     - Graceful failure without system impact

   Corruption Handling Tests:
     1. Test ZIP corruption detection
     2. Verify XML malformation handling
     3. Check partial recovery capabilities
     4. Validate error reporting quality
   ```

2. **Database Integrity Issue Handling**
   ```
   Test Scenario: SQLite databases with integrity violations
   Source: Databases with constraint violations, schema corruption
   Expected Outcome:
     - Database integrity validation
     - Clear error reporting with technical details
     - Suggested recovery procedures
     - Safe handling without data corruption risk

   Integrity Testing:
     1. Test constraint violation detection
     2. Check schema corruption handling
     3. Verify recovery procedure suggestions
     4. Validate safe failure behavior
   ```

### 4.3 Resource Exhaustion Testing

#### Low Memory Conditions
1. **Memory-Constrained Environment Testing**
   ```
   Test Scenario: Import operations under severe memory constraints
   Source: Large files with artificially limited memory availability
   Expected Outcome:
     - Intelligent memory management
     - Operation prioritization under constraints
     - Clear user notification of limitations
     - Stable system behavior within limits

   Memory Constraint Tests:
     1. Limit available memory artificially
     2. Test operation prioritization
     3. Verify stability under constraints
     4. Check user notification quality
   ```

2. **Storage Space Management**
   ```
   Test Scenario: Import operations with limited disk space
   Source: Large files with minimal free disk space
   Expected Outcome:
     - Space requirement calculation
     - Early warning before space exhaustion
     - Alternative processing strategies
     - Proper cleanup and space management

   Storage Management Validation:
     1. Test space requirement calculation
     2. Verify early warning systems
     3. Check alternative processing methods
     4. Validate cleanup effectiveness
   ```

## 5. User Experience Integration Tests

### 5.1 Progress Reporting Accuracy Testing

#### Real-Time Import Progress Visualization
1. **Multi-Stage Progress Reporting**
   ```
   Test Scenario: Complex import with multiple distinct processing stages
   Source: Large Excel file with extraction, parsing, transformation stages
   Expected Outcome:
     - Clear stage identification and progress
     - Accurate time estimation and completion prediction
     - Responsive progress bar updates
     - Informative stage descriptions

   Progress Reporting Tests:
     1. Verify stage identification accuracy
     2. Test time estimation precision
     3. Check progress bar responsiveness
     4. Validate stage description clarity
   ```

2. **Concurrent Operation Progress Aggregation**
   ```
   Test Scenario: Multiple simultaneous imports with combined progress
   Source: 3+ different file types importing concurrently
   Expected Outcome:
     - Accurate aggregated progress calculation
     - Individual operation progress visibility
     - Overall completion time estimation
     - Clear operation status indicators

   Aggregation Tests:
     1. Verify aggregate progress calculation
     2. Check individual operation visibility
     3. Test completion time accuracy
     4. Validate status indicator clarity
   ```

### 5.2 Cancel and Retry Workflows

#### User-Initiated Operation Management
1. **Graceful Cancellation Testing**
   ```
   Test Scenario: User cancellation during various import stages
   Source: Long-running imports at different completion percentages
   Expected Outcome:
     - Immediate response to cancellation request
     - Clean operation termination
     - Proper progress state preservation
     - Clear cancellation confirmation

   Cancellation Tests:
     1. Test cancellation during each major stage
     2. Verify immediate response time
     3. Check state preservation
     4. Validate confirmation messaging
   ```

2. **Retry and Recovery Workflows**
   ```
   Test Scenario: Failed import retry with user guidance
   Source: Failed imports with various error conditions
   Expected Outcome:
     - Clear error explanation and guidance
     - Intelligent retry suggestions
     - Modified retry with user input
     - Learning from previous failures

   Retry Workflow Tests:
     1. Test error explanation clarity
     2. Verify retry suggestion relevance
     3. Check guided retry effectiveness
     4. Validate failure learning integration
   ```

### 5.3 Error Reporting and User Guidance

#### Comprehensive Error Communication
1. **Technical Error Translation**
   ```
   Test Scenario: Technical errors translated to user-friendly guidance
   Source: Various technical failure scenarios
   Expected Outcome:
     - Non-technical error explanations
     - Actionable resolution steps
     - Technical details available when needed
     - Clear next steps and alternatives

   Error Communication Tests:
     1. Verify error message clarity
     2. Test actionability of guidance
     3. Check technical detail accessibility
     4. Validate next steps clarity
   ```

2. **Error Context and Impact Explanation**
   ```
   Test Scenario: Error impact explanation with context
   Source: Partial failures and data quality issues
   Expected Outcome:
     - Clear impact assessment
     - Context-specific guidance
     - Risk explanation and mitigation
     - Alternative approach suggestions

   Context Testing:
     1. Test impact assessment accuracy
     2. Verify context relevance
     3. Check risk explanation clarity
     4. Validate alternative suggestions
   ```

## Integration Testing Implementation Framework

### Automated Testing Infrastructure
```swift
class IntegrationTestSuite {
    func executeComprehensiveIntegrationTests() async throws {
        let crossSystemTests = await executeCrossSystemTests()
        let workflowTests = await executeWorkflowTests()
        let performanceTests = await executePerformanceTests()
        let errorHandlingTests = await executeErrorHandlingTests()
        let userExperienceTests = await executeUserExperienceTests()

        let overallResults = aggregateTestResults([
            crossSystemTests,
            workflowTests,
            performanceTests,
            errorHandlingTests,
            userExperienceTests
        ])

        generateIntegrationReport(overallResults)
    }
}
```

### Test Success Criteria Summary
- Cross-system integration: ≥90% accuracy in all transformation tests
- Workflow integration: ≥95% end-to-end success rate
- Performance integration: All performance targets met under stress
- Error handling integration: ≥95% graceful error recovery rate
- User experience integration: ≥90% user workflow completion rate

This comprehensive integration testing framework ensures robust, reliable, and user-friendly import operations across all supported formats and enterprise-scale scenarios.