---
phase: 70-integration
plan: 02
type: execute
wave: 2
depends_on: ["70-01"]
files_modified:
  - src/etl/alto-importer.ts
  - src/etl/__tests__/alto-importer.test.ts
autonomous: true

must_haves:
  truths:
    - "AltoImporter extends BaseImporter and returns CanonicalNode[]"
    - "Existing alto-importer API preserved for backward compatibility"
    - "CanonicalNodeSchema validates all imported nodes"
  artifacts:
    - path: "src/etl/alto-importer.ts"
      provides: "Refactored importer using CanonicalNode pipeline"
      exports: ["AltoImporter", "importAltoFile"]
    - path: "src/etl/__tests__/alto-importer.test.ts"
      provides: "Updated tests validating CanonicalNode output"
      contains: "CanonicalNodeSchema"
  key_links:
    - from: "src/etl/alto-importer.ts"
      to: "src/etl/importers/BaseImporter.ts"
      via: "extends BaseImporter"
      pattern: "extends BaseImporter"
    - from: "src/etl/alto-importer.ts"
      to: "src/etl/types/canonical.ts"
      via: "CanonicalNode type and schema"
      pattern: "CanonicalNode"
---

<objective>
Migrate alto-importer to use CanonicalNode pipeline.

Purpose: Unify all importers under the same CanonicalNode interface for consistent database insertion.
Output: AltoImporter class extending BaseImporter, returning validated CanonicalNode[].
</objective>

<execution_context>
@/Users/mshaler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mshaler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/70-integration/70-01-SUMMARY.md

@src/etl/alto-importer.ts
@src/etl/importers/BaseImporter.ts
@src/etl/types/canonical.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor alto-importer to extend BaseImporter</name>
  <files>src/etl/alto-importer.ts</files>
  <action>
Refactor alto-importer to use CanonicalNode pipeline:

1. Import new dependencies:
   ```typescript
   import { BaseImporter, type ImportResult, type FileSource } from './importers/BaseImporter';
   import { CanonicalNode, CanonicalNodeSchema, SQL_COLUMN_MAP } from './types/canonical';
   import { parseFrontmatter } from './parsers/frontmatter-parser';
   import { generateDeterministicSourceId } from './id-generation/deterministic';
   ```

2. Create AltoImporter class:
   ```typescript
   export class AltoImporter extends BaseImporter {
     readonly supportedExtensions = ['.md', '.markdown'];
     readonly name = 'alto-importer';

     async import(source: FileSource): Promise<CanonicalNode[]> {
       const nodes: CanonicalNode[] = [];

       try {
         const node = this.parseAltoFile(source.filename, source.content);
         const validated = CanonicalNodeSchema.safeParse(node);

         if (validated.success) {
           nodes.push(validated.data);
         } else {
           throw new Error(`Validation failed: ${validated.error.message}`);
         }
       } catch (error) {
         throw error; // Let coordinator handle errors
       }

       return nodes;
     }

     private parseAltoFile(filename: string, content: string): CanonicalNode {
       // Use existing parseFrontmatter
       const { data: frontmatter, content: body } = parseFrontmatter(content);

       // Map frontmatter to CanonicalNode fields
       return this.mapToCanonicalNode(frontmatter, body, filename);
     }

     private mapToCanonicalNode(
       frontmatter: Record<string, unknown>,
       body: string,
       filename: string
     ): CanonicalNode {
       const id = generateDeterministicSourceId(filename, frontmatter, 'alto');

       return {
         id,
         nodeType: 'note',
         name: String(frontmatter.title || frontmatter.name || filename),
         content: body,
         summary: null,
         source: 'alto-index',
         sourceId: id,
         sourceUrl: null,

         // Location
         latitude: null,
         longitude: null,
         locationName: frontmatter.location ? String(frontmatter.location) : null,
         locationAddress: null,

         // Time (ISO 8601 strings)
         createdAt: frontmatter.created
           ? new Date(String(frontmatter.created)).toISOString()
           : new Date().toISOString(),
         modifiedAt: frontmatter.modified
           ? new Date(String(frontmatter.modified)).toISOString()
           : new Date().toISOString(),
         dueAt: frontmatter.due ? new Date(String(frontmatter.due)).toISOString() : null,
         completedAt: null,
         eventStart: null,
         eventEnd: null,

         // Category
         folder: frontmatter.folder ? String(frontmatter.folder) : null,
         tags: Array.isArray(frontmatter.tags) ? frontmatter.tags.map(String) : [],
         status: frontmatter.status ? String(frontmatter.status) : null,

         // Hierarchy
         priority: typeof frontmatter.priority === 'number' ? frontmatter.priority : 0,
         importance: 0,
         sortOrder: 0,

         // Grid
         gridX: 0,
         gridY: 0,

         // Lifecycle
         version: 1,
         deletedAt: null,

         // Dynamic properties (keys not in SQL_COLUMN_MAP)
         properties: this.extractUnknownProperties(frontmatter),
       };
     }

     private extractUnknownProperties(
       frontmatter: Record<string, unknown>
     ): Record<string, unknown> {
       // Use SQL_COLUMN_MAP to get complete list of canonical fields
       // Plus the explicit frontmatter key aliases that map to canonical fields
       const canonicalKeys = new Set(Object.keys(SQL_COLUMN_MAP));
       const frontmatterAliases = new Set([
         'title',      // -> name
         'created',    // -> createdAt
         'modified',   // -> modifiedAt
         'due',        // -> dueAt
         'location',   // -> locationName
       ]);

       const unknown: Record<string, unknown> = {};
       for (const [key, value] of Object.entries(frontmatter)) {
         // Skip if it's a canonical field OR a known alias
         if (!canonicalKeys.has(key) && !frontmatterAliases.has(key)) {
           unknown[key] = value;
         }
       }

       return unknown; // Return empty object rather than undefined (schema default is {})
     }
   }
   ```

3. Preserve backward-compatible function wrapper:
   ```typescript
   import { insertCanonicalNodes } from './database/insertion';
   import type { Database } from 'sql.js';

   /**
    * Backward-compatible wrapper for legacy code.
    *
    * ARCHITECTURE NOTE: This wrapper exists for backward compatibility only.
    * The clean architecture is:
    *   AltoImporter.import() -> CanonicalNode[] -> caller handles insertion
    *
    * New code should use:
    *   const importer = new AltoImporter();
    *   const nodes = await importer.import({ filename, content });
    *   await insertCanonicalNodes(db, nodes);
    */
   export async function importAltoFile(
     db: Database,
     filename: string,
     content: string
   ): Promise<{ nodeId: string; errors: string[] }> {
     const importer = new AltoImporter();
     const nodes = await importer.import({ filename, content });

     if (nodes.length > 0) {
       // Insert using new utility
       const insertResult = await insertCanonicalNodes(db, nodes);
       return {
         nodeId: nodes[0].id,
         errors: insertResult.errors.map(e => e.error)
       };
     }

     return { nodeId: '', errors: ['No nodes imported'] };
   }
   ```

4. Remove old mapToNodeRecord and direct SQL insertion code.
  </action>
  <verify>
npm run typecheck (zero errors)
grep "extends BaseImporter" src/etl/alto-importer.ts (found)
  </verify>
  <done>
AltoImporter extends BaseImporter, returns CanonicalNode[], backward-compatible wrapper documented as legacy
  </done>
</task>

<task type="auto">
  <name>Task 2: Update alto-importer tests</name>
  <files>src/etl/__tests__/alto-importer.test.ts</files>
  <action>
Update tests to validate CanonicalNode output:

1. Test "AltoImporter.import returns CanonicalNode array":
   - Create test markdown with frontmatter
   - Call importer.import({ filename, content })
   - Verify result is array with 1 element
   - Verify node has required CanonicalNode fields

2. Test "AltoImporter validates against CanonicalNodeSchema":
   - Import valid file
   - Verify CanonicalNodeSchema.safeParse(result[0]).success === true

3. Test "AltoImporter maps frontmatter to LATCH fields":
   - Create markdown with: title, created, folder, tags, priority
   - Import and verify each field maps correctly:
     - title -> name
     - created -> createdAt (ISO 8601 string)
     - folder -> folder
     - tags -> tags (string[])
     - priority -> priority (number)

4. Test "AltoImporter extracts unknown properties":
   - Create markdown with custom frontmatter: `custom_key: value`
   - Import and verify result[0].properties = { custom_key: 'value' }
   - Verify known keys (title, created, etc.) are NOT in properties

5. Test "importAltoFile backward compatibility":
   - Use legacy function with database
   - Verify node inserted correctly
   - Verify nodeId returned

6. Test "AltoImporter handles malformed frontmatter":
   - Invalid YAML
   - Verify error thrown (let coordinator catch it)
  </action>
  <verify>
npm run test -- alto-importer --run (all tests pass)
  </verify>
  <done>
Test suite validates CanonicalNode output, schema compliance, LATCH mapping, and backward compatibility
  </done>
</task>

</tasks>

<verification>
1. Run typecheck: `npm run typecheck`
2. Run tests: `npm run test -- alto-importer --run`
3. Integration test: Import a real .md file and verify CanonicalNode[] output
4. Verify backward compatibility: Old importAltoFile() still works
</verification>

<success_criteria>
- AltoImporter extends BaseImporter
- import() returns CanonicalNode[] validated by schema
- LATCH fields mapped correctly from frontmatter
- Unknown properties extracted using SQL_COLUMN_MAP for complete known field list
- Backward-compatible importAltoFile() wrapper preserved with architecture documentation
- All tests pass, zero TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/70-integration/70-02-SUMMARY.md`
</output>
